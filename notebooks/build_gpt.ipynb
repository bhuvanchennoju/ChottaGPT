{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building GPT from Scatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n",
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "# downloading toy dataset \n",
    "import os\n",
    "import requests\n",
    "\n",
    "WORK_dir = '/n/projects/kc2819/projects/ChotaLLM'\n",
    "DATA_dir =  os.path.join(WORK_dir, 'data')\n",
    "SRC_dir = os.path.join(WORK_dir, 'src')\n",
    "\n",
    "input_file_path = os.path.join(DATA_dir,'shakespeare', 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(text[:1000])\n",
    "    print('text length:', len(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple index based tokenization\n",
    "\n",
    "* get unique characters --> vocabulary \n",
    "* create a simple tokenizer to convert a string to number with vocabulary.\n",
    "* Todo that useing simple encoder decoder kind fo character mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizer:\n",
    "\n",
    "    '''\n",
    "    Simple tokenizer that encode decode character level tokenization.\n",
    "    Alternatives could be google sentencepiece, huggingface tokenizers, openai Tiktoken library.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self,text):\n",
    "        self.unique_chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.unique_chars)\n",
    "        # strign to index mapping\n",
    "        self.str_to_idx = {chr: idx for idx,chr in enumerate(self.unique_chars)}\n",
    "        # index to string mapping\n",
    "        self.idx_to_str = {idx: chr for idx,chr in enumerate(self.unique_chars)}\n",
    "\n",
    "        # adding padding and unknown token\n",
    "        self.str_to_idx['<PAD>'] = self.vocab_size\n",
    "        self.idx_to_str[self.vocab_size] = '<PAD>'\n",
    "        self.unique_chars.append('<PAD>')\n",
    "\n",
    "        self.str_to_idx['<UNK>'] = self.vocab_size + 1\n",
    "        self.idx_to_str[self.vocab_size + 1] = '<UNK>'\n",
    "        self.unique_chars.append('<UNK>')\n",
    "\n",
    "        self.vocab_size += 2\n",
    "\n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        this function takes a string and look up in the string to index mapping, and return the list of indices.\n",
    "        '''\n",
    " \n",
    "        return [self.str_to_idx[chr] if chr in self.unique_chars else self.str_to_idx['<UNK>'] for chr in text]\n",
    "        \n",
    "        \n",
    "    def decode(self, indices):\n",
    "        '''\n",
    "        this function takes a list of indices and look up in the index to string mapping, and return the string.\n",
    "        '''\n",
    "        return ''.join([self.idx_to_str[idx] for idx in indices]) \n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def get_unique_chars(self):\n",
    "        return self.unique_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 67\n",
      "unique chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<PAD><UNK>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = simpleTokenizer(text)\n",
    "print('vocab size:', tokenizer.get_vocab_size())\n",
    "print('unique chars:', ''.join(tokenizer.get_unique_chars()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded text: [46, 43, 50, 50, 53, 6, 1, 21, 25, 1, 14, 46, 59, 60, 39, 52, 6, 1, 39, 57, 42, 44, 50, 39, 57, 42, 44, 1, 4, 4, 66, 4, 4, 66, 66, 66, 66, 66, 66]\n",
      "decoded text: hello, IM Bhuvan, asdflasdf &&<UNK>&&<UNK><UNK><UNK><UNK><UNK><UNK>\n"
     ]
    }
   ],
   "source": [
    "# test cast\n",
    "encoded_text = tokenizer.encode('hello, IM Bhuvan, asdflasdf &&*&&)))())')\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print('encoded text:', encoded_text)\n",
    "print('decoded text:', decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded data shape, type \n",
      " torch.Size([1115394]) torch.int64\n",
      "encoded data: \n",
      " tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "decoded data: \n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(tokenizer.encode(text),dtype = torch.long)\n",
    "print('encoded data shape, type \\n' ,data.shape,data.dtype)\n",
    "print( 'encoded data: \\n', data[:100])\n",
    "print('decoded data: \\n', tokenizer.decode(data[:100].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "block size or context length is a small chunk of data in a a sequencial fashion. When we feed this block of string into a transformer, with respect to string{1 -> n} n+1th character is the target for the tranfomer. From the lecture chunk_size or block_size = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m context \u001b[39m=\u001b[39m y[:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m target \u001b[39m=\u001b[39m y[i]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcontext:\u001b[39m\u001b[39m'\u001b[39m, tokenizer\u001b[39m.\u001b[39mdecode(context\u001b[39m.\u001b[39mtolist()), \u001b[39m'\u001b[39m\u001b[39mtarget:\u001b[39m\u001b[39m'\u001b[39m, tokenizer\u001b[39m.\u001b[39;49mdecode([target]))\n",
      "\u001b[1;32m/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, indices):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m    this function takes a list of indices and look up in the index to string mapping, and return the string.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/projects/kc2819/projects/ChotaLLM/notebooks/build_gpt.ipynb#Y165sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49midx_to_str[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(47)"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]  # block with 0 -> block_size for the training input \n",
    "y = train_data[1:block_size+1] # block with 1 -> block_size+1 for the training target, shifted by 1 from the input\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f'for context:{context},target is {target} \\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
